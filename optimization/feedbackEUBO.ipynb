{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5d28782",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'shape_to_str' from 'botorch.logging' (/home/asj53/.conda/envs/research_env/lib/python3.9/site-packages/botorch/logging.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01macquisition\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulti_objective\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhypervolume_knowledge_gradient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     qHypervolumeKnowledgeGradient,\n\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01macquisition\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulti_objective\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpredictive_entropy_search\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      5\u001b[0m     qMultiObjectivePredictiveEntropySearch,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SobolQMCNormalSampler\n",
      "File \u001b[0;32m~/.conda/envs/research_env/lib/python3.9/site-packages/botorch/acquisition/multi_objective/hypervolume_knowledge_gradient.py:30\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01macquisition\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01macquisition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     25\u001b[0m     AcquisitionFunction,\n\u001b[1;32m     26\u001b[0m     OneShotAcquisitionFunction,\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01macquisition\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcost_aware\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CostAwareUtility\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01macquisition\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecoupled\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DecoupledAcquisitionFunction\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01macquisition\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mknowledge_gradient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ProjectedAcquisitionFunction\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01macquisition\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulti_objective\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmonte_carlo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     33\u001b[0m     MultiObjectiveMCAcquisitionFunction,\n\u001b[1;32m     34\u001b[0m     qExpectedHypervolumeImprovement,\n\u001b[1;32m     35\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/research_env/lib/python3.9/site-packages/botorch/acquisition/decoupled.py:19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BotorchWarning\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BotorchTensorDimensionError\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m shape_to_str\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelList\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'shape_to_str' from 'botorch.logging' (/home/asj53/.conda/envs/research_env/lib/python3.9/site-packages/botorch/logging.py)"
     ]
    }
   ],
   "source": [
    "from botorch.acquisition.multi_objective.hypervolume_knowledge_gradient import (\n",
    "    qHypervolumeKnowledgeGradient,\n",
    ")\n",
    "from botorch.acquisition.multi_objective.predictive_entropy_search import (\n",
    "    qMultiObjectivePredictiveEntropySearch,\n",
    ")\n",
    "from botorch.sampling import SobolQMCNormalSampler\n",
    "from botorch.utils.transforms import unnormalize\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4083d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7.0\n"
     ]
    }
   ],
   "source": [
    "import botorch, gpytorch, torch\n",
    "print(botorch.__version__)   # -> 0.13.0  (or newer)\n",
    "print(gpytorch.__version__)  # -> 1.11 or 1.12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbe4724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from botorch.test_functions.multi_objective import DTLZ2\n",
    "from botorch.utils.sampling import draw_sobol_samples\n",
    "import torch\n",
    "from botorch.models.gp_regression import SingleTaskGP\n",
    "from botorch.models.pairwise_gp import PairwiseGP, PairwiseLaplaceMarginalLogLikelihood\n",
    "from botorch.fit import fit_gpytorch_model\n",
    "from botorch.acquisition.monte_carlo import qExpectedImprovement\n",
    "from botorch.optim.optimize import optimize_acqf\n",
    "from botorch.sampling import SobolQMCNormalSampler\n",
    "from gpytorch.mlls.exact_marginal_log_likelihood import ExactMarginalLogLikelihood\n",
    "\n",
    "def fit_pref_model(Y: torch.Tensor, comps: torch.Tensor) -> PairwiseGP:\n",
    "    # detach Y so that no upstream graph remains\n",
    "    Y_det = Y.detach().clone()\n",
    "    model = PairwiseGP(\n",
    "        train_X=Y_det,\n",
    "        train_targets=comps,\n",
    "        input_transform=Normalize(d=Y_det.size(-1)),\n",
    "    )\n",
    "    mll = PairwiseLaplaceMarginalLogLikelihood(model.likelihood, model)\n",
    "    fit_gpytorch_model(mll)\n",
    "    return model\n",
    "\n",
    "# --- 1. Define the problem and set a seed for reproducibility ---\n",
    "torch.manual_seed(0)\n",
    "problem = DTLZ2(dim=5, num_objectives=4)\n",
    "\n",
    "# --- 2. Generate initial experimental data (X_obs, Y_obs) ---\n",
    "#    8 quasi-random Sobol points in [0,1]^5\n",
    "X_obs = draw_sobol_samples(bounds=problem.bounds, n=1, q=8).squeeze(0)       # Tensor of shape [8, 5]\n",
    "Y_obs = problem(X_obs)                                                       # Tensor of shape [8, 4]\n",
    "from typing import Optional, Tuple\n",
    "from botorch.models.transforms.input import Normalize\n",
    "from botorch.models.transforms.outcome import Standardize\n",
    "# --- 3. Fit the outcome GP to (X_obs, Y_obs) ---\n",
    "outcome_model = fit_outcome_model(X_obs, Y_obs)\n",
    "#    → outcome_model is now a SingleTaskGP trained on those 8 data points\n",
    "\n",
    "def neg_l1_dist(Y: torch.Tensor, X: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "    \"\"\"Negative L1 distance from a Pareto optimal points\"\"\"\n",
    "    if len(Y.shape) == 1:\n",
    "        Y = Y.unsqueeze(0)\n",
    "    dist = torch.cdist(\n",
    "        Y, torch.full(Y.shape[-1:], fill_value=0.5, dtype=Y.dtype).unsqueeze(0), p=1\n",
    "    ).squeeze(-1)\n",
    "    return -dist\n",
    "def fit_outcome_model(X: torch.Tensor, Y: torch.Tensor) -> SingleTaskGP:\n",
    "    \"\"\"Fit the outcome model f\"\"\"\n",
    "    outcome_model = SingleTaskGP(\n",
    "        train_X=X,\n",
    "        train_Y=Y,\n",
    "        input_transform=Normalize(d=X.shape[-1]),\n",
    "        outcome_transform=Standardize(m=Y.shape[-1]),\n",
    "    )\n",
    "    mll = ExactMarginalLogLikelihood(outcome_model.likelihood, outcome_model)\n",
    "    fit_gpytorch_model(mll)\n",
    "    return outcome_model\n",
    "util_func = neg_l1_dist\n",
    "\n",
    "\n",
    "def fit_pref_model(Y: torch.Tensor, comps: torch.Tensor) -> PairwiseGP:\n",
    "    \"\"\"Fit the preference model g.\"\"\"\n",
    "    model = PairwiseGP(Y, comps, input_transform=Normalize(d=Y.shape[-1]))\n",
    "    mll = PairwiseLaplaceMarginalLogLikelihood(model.likelihood, model)\n",
    "    fit_gpytorch_model(mll)\n",
    "    return model\n",
    "\n",
    "\n",
    "def gen_rand_X(problem, n: int) -> torch.Tensor:\n",
    "    \"\"\"Generate n quasi-random Sobol points in the design space.\"\"\"\n",
    "    return draw_sobol_samples(bounds=problem.bounds, n=1, q=n).squeeze(0)\n",
    "\n",
    "\n",
    "def generate_random_exp_data(problem, n: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Generate n observations of (X, Y) Pairs\"\"\"\n",
    "    X = gen_rand_X(problem, n)\n",
    "    Y = problem(X)\n",
    "    return X, Y\n",
    "\n",
    "def generate_random_pref_data(\n",
    "    outcome_model: SingleTaskGP, n: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    X = gen_rand_X(problem, 2 * n)\n",
    "    # Draw *detached* samples from the GP posterior:\n",
    "    with torch.no_grad():\n",
    "        # use .sample() to avoid a reparameterized graph, or\n",
    "        # use .rsample().detach() if you really need rsample\n",
    "        Y = outcome_model.posterior(X).sample().squeeze(0)\n",
    "    util = util_func(Y)\n",
    "    comps = gen_comps(util)\n",
    "    return Y, comps\n",
    "\n",
    "\n",
    "def gen_comps(util: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Given an 1-d tensor of utility, create pairwise comparisons between adjacent items.\"\"\"\n",
    "    util = util.reshape(-1, 2)\n",
    "    comps = torch.arange(util.numel()).reshape(-1, 2)\n",
    "    flip = util[:, 0] < util[:, 1]\n",
    "    comps[flip, [0]], comps[flip, [1]] = comps[flip, [1]], comps[flip, [0]]\n",
    "\n",
    "    return comps\n",
    "# --- 4. Generate an initial set of preference‐comparison data ---\n",
    "#    Here “n=4” means we’ll sample 2*n = 8 new outcomes and get 4 pairwise comps\n",
    "Y_pref_baseline, pair_indices = generate_random_pref_data(outcome_model, n=4)\n",
    "#    Y_pref_baseline: Tensor of shape [8, 4] (the sampled outcome vectors)\n",
    "#    pair_indices:     Tensor of shape [4, 2] (each row is a pair of indices into Y_pref_baseline)\n",
    "\n",
    "# --- 5. Fit the preference GP to those comparisons ---\n",
    "pref_model = fit_pref_model(Y_pref_baseline, pair_indices)\n",
    "from botorch.acquisition import LearnedObjective\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Posterior mean utility for each baseline Y\n",
    "    baseline_util = pref_model.posterior(Y_pref_baseline).mean.squeeze(-1)\n",
    "best_f = baseline_util.max()  # scalar Tensor\n",
    "pref_obj = LearnedObjective(pref_model=pref_model)\n",
    "# Assume we have:\n",
    "# - outcome_model: SingleTaskGP fitted to (X_obs, Y_obs)\n",
    "# - pref_model: PairwiseGP fitted to preference data (Y_obs, pair_indices)\n",
    "# - X_obs: previously evaluated inputs for the outcome model (n x d tensor)\n",
    "# - Y_obs: corresponding outcomes (n x k tensor)\n",
    "# - Y_pref_baseline: set of outcome vectors used as the baseline for pref GP (m x k tensor)\n",
    "\n",
    "# 3. Configure Monte Carlo sampler for outcome model (f) \n",
    "\n",
    "outcome_sampler = SobolQMCNormalSampler(num_samples = 100 )\n",
    "\n",
    "# 4. Instantiate qNEIUU acquisition function\n",
    "acqf_qnei_uu = qExpectedImprovement(\n",
    "    model=outcome_model,     # SingleTaskGP on (X_obs, Y_obs)\n",
    "    best_f=best_f,           # current best utility\n",
    "    sampler=outcome_sampler, # QMC sampler for f\n",
    "    objective=pref_obj,      # LearnedObjective wrapping pref_model\n",
    "    # posterior_transform=None, X_pending=None, constraints=None, eta=1e-3 are all optional\n",
    ")\n",
    "\n",
    "from botorch.optim import optimize_acqf\n",
    "d = 5\n",
    "bounds = torch.stack([torch.zeros(d), torch.ones(d)])  # example [0,1]^d\n",
    "candidates, acq_vals = optimize_acqf(\n",
    "    acq_function=acqf_qnei_uu,\n",
    "    bounds=bounds,\n",
    "    q=1,              # batch size\n",
    "    num_restarts=8,\n",
    "    raw_samples=64,\n",
    "    options={\"batch_limit\": 4},\n",
    "    sequential=False, # joint batch optimization\n",
    ")\n",
    "\n",
    "\n",
    "# Run your expensive evaluation (or simulator) to get true outcomes\n",
    "Y_new = problem(candidates)  # or f_true(candidates)\n",
    "# Append new points to your observed data\n",
    "X_obs = torch.cat([X_obs, candidates], dim=0)   # now (n+q)×d\n",
    "Y_obs = torch.cat([Y_obs, Y_new],      dim=0)   # now (n+q)×k\n",
    "# Example: compare each new Y_new[i] against a random existing outcome\n",
    "# (you’d replace this with your actual DM interface)\n",
    "pairs = []\n",
    "for i in range(Y_new.shape[0]):\n",
    "    # pick a random baseline index\n",
    "    j = torch.randint(0, Y_pref_baseline.shape[0], (1,)).item()\n",
    "    # create a pair [baseline_idx, new_idx]\n",
    "    pairs.append([j, Y_pref_baseline.shape[0] + i])\n",
    "pairs = torch.tensor(pairs, dtype=torch.long)  # shape (q, 2)\n",
    "\n",
    "# Augment the preference dataset\n",
    "Y_pref_baseline = torch.cat([Y_pref_baseline, Y_new], dim=0)   # (m+q)×k\n",
    "pair_indices     = torch.cat([pair_indices, pairs],     dim=0)  # (#pairs_old+q)×2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a629a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48565bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96c3a02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8b4f96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3d625655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PairwiseLaplaceMarginalLogLikelihood(\n",
       "  (likelihood): PairwiseProbitLikelihood()\n",
       "  (model): PairwiseGP(\n",
       "    (input_transform): Normalize()\n",
       "    (likelihood): PairwiseProbitLikelihood()\n",
       "    (mean_module): ConstantMean()\n",
       "    (covar_module): ScaleKernel(\n",
       "      (base_kernel): RBFKernel(\n",
       "        (lengthscale_prior): GammaPrior()\n",
       "        (raw_lengthscale_constraint): Positive()\n",
       "      )\n",
       "      (outputscale_prior): SmoothedBoxPrior()\n",
       "      (raw_outputscale_constraint): Positive()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Refit outcome GP on the expanded (X_obs, Y_obs)\n",
    "outcome_model = SingleTaskGP(\n",
    "    train_X=X_obs,\n",
    "    train_Y=Y_obs,\n",
    "    # … same transforms as before …\n",
    ")\n",
    "fit_gpytorch_model(ExactMarginalLogLikelihood(outcome_model.likelihood, outcome_model))\n",
    "\n",
    "fit_gpytorch_model(PairwiseLaplaceMarginalLogLikelihood(pref_model.likelihood, pref_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1ec48ceb",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 80\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m#    c) Initial preference data (e.g., 4 comparisons)\u001b[39;00m\n\u001b[1;32m     79\u001b[0m Y_pref, pair_indices \u001b[38;5;241m=\u001b[39m generate_random_pref_data(outcome_model, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m---> 80\u001b[0m pref_model \u001b[38;5;241m=\u001b[39m \u001b[43mfit_pref_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_pref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpair_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# 3) BOPE iteration parameters\u001b[39;00m\n\u001b[1;32m     83\u001b[0m N_ITER \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m               \u001b[38;5;66;03m# total BOPE cycles\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[45], line 60\u001b[0m, in \u001b[0;36mfit_pref_model\u001b[0;34m(Y, comps)\u001b[0m\n\u001b[1;32m     58\u001b[0m m \u001b[38;5;241m=\u001b[39m PairwiseGP(Y, comps, input_transform\u001b[38;5;241m=\u001b[39mNormalize(d\u001b[38;5;241m=\u001b[39mY\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)))\n\u001b[1;32m     59\u001b[0m mll \u001b[38;5;241m=\u001b[39m PairwiseLaplaceMarginalLogLikelihood(m\u001b[38;5;241m.\u001b[39mlikelihood, m)\n\u001b[0;32m---> 60\u001b[0m \u001b[43mfit_gpytorch_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmll\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m m\n",
      "File \u001b[0;32m~/.conda/envs/research_env/lib/python3.9/site-packages/botorch/fit.py:130\u001b[0m, in \u001b[0;36mfit_gpytorch_model\u001b[0;34m(mll, optimizer, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     sample_all_priors(mll\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     mll, _ \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrack_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NotPSDError:\n\u001b[1;32m    132\u001b[0m     retry \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/research_env/lib/python3.9/site-packages/botorch/optim/fit.py:241\u001b[0m, in \u001b[0;36mfit_gpytorch_scipy\u001b[0;34m(mll, bounds, method, options, track_iterations, approx_mll, scipy_objective, module_to_array_func, module_from_array_func)\u001b[0m\n\u001b[1;32m    238\u001b[0m cb \u001b[38;5;241m=\u001b[39m store_iteration \u001b[38;5;28;01mif\u001b[39;00m track_iterations \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m gpt_settings\u001b[38;5;241m.\u001b[39mfast_computations(log_prob\u001b[38;5;241m=\u001b[39mapprox_mll):\n\u001b[0;32m--> 241\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscipy_objective\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperty_dict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     iterations \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m track_iterations:\n",
      "File \u001b[0;32m~/.conda/envs/research_env/lib/python3.9/site-packages/scipy/optimize/_minimize.py:713\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    710\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    711\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 713\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    716\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m    717\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m~/.conda/envs/research_env/lib/python3.9/site-packages/scipy/optimize/_lbfgsb_py.py:369\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    363\u001b[0m task_str \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFG\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[0;32m--> 369\u001b[0m     f, g \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW_X\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[1;32m    372\u001b[0m     n_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/research_env/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:296\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx):\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x_impl(x)\n\u001b[0;32m--> 296\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_grad()\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n",
      "File \u001b[0;32m~/.conda/envs/research_env/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:262\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/research_env/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:163\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/research_env/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:145\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[0;32m~/.conda/envs/research_env/lib/python3.9/site-packages/scipy/optimize/_optimize.py:78\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m     77\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[0;32m~/.conda/envs/research_env/lib/python3.9/site-packages/scipy/optimize/_optimize.py:72\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(x \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 72\u001b[0m     fg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/research_env/lib/python3.9/site-packages/botorch/optim/utils.py:218\u001b[0m, in \u001b[0;36m_scipy_objective_and_grad\u001b[0;34m(x, mll, property_dict)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _handle_numerical_errors(error\u001b[38;5;241m=\u001b[39me, x\u001b[38;5;241m=\u001b[39mx)\n\u001b[0;32m--> 218\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m param_dict \u001b[38;5;241m=\u001b[39m OrderedDict(mll\u001b[38;5;241m.\u001b[39mnamed_parameters())\n\u001b[1;32m    220\u001b[0m grad \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.conda/envs/research_env/lib/python3.9/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/research_env/lib/python3.9/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/research_env/lib/python3.9/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[0;32m~/.conda/envs/research_env/lib/python3.9/site-packages/torch/autograd/function.py:307\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    305\u001b[0m     )\n\u001b[1;32m    306\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/research_env/lib/python3.9/site-packages/linear_operator/functions/_matmul.py:35\u001b[0m, in \u001b[0;36mMatmul.backward\u001b[0;34m(ctx, grad_output)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(ctx, grad_output):\n\u001b[0;32m---> 35\u001b[0m     rhs \u001b[38;5;241m=\u001b[39m \u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaved_tensors\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     36\u001b[0m     matrix_args \u001b[38;5;241m=\u001b[39m ctx\u001b[38;5;241m.\u001b[39msaved_tensors[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m     37\u001b[0m     rhs_shape \u001b[38;5;241m=\u001b[39m rhs\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from botorch.test_functions.multi_objective import DTLZ2\n",
    "from botorch.utils.sampling import draw_sobol_samples\n",
    "from botorch.models.gp_regression import SingleTaskGP\n",
    "from botorch.models.pairwise_gp import PairwiseGP, PairwiseLaplaceMarginalLogLikelihood\n",
    "from botorch.models.transforms.input import Normalize\n",
    "from botorch.models.transforms.outcome import Standardize\n",
    "from botorch.fit import fit_gpytorch_model\n",
    "from gpytorch.mlls.exact_marginal_log_likelihood import ExactMarginalLogLikelihood\n",
    "from botorch.acquisition.monte_carlo import qExpectedImprovement\n",
    "from botorch.acquisition.objective import LearnedObjective\n",
    "from botorch.optim import optimize_acqf\n",
    "from botorch.sampling import SobolQMCNormalSampler\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# -- User‐defined utilities --------------------------------------------------\n",
    "\n",
    "def neg_l1_dist(Y: torch.Tensor, X: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "    if Y.dim() == 1:\n",
    "        Y = Y.unsqueeze(0)\n",
    "    dist = torch.cdist(\n",
    "        Y, torch.full((1, Y.size(-1)), 0.5, dtype=Y.dtype), p=1\n",
    "    ).squeeze(-1)\n",
    "    return -dist\n",
    "\n",
    "def gen_comps(util: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Pairwise indices so that the higher‐utility item comes first.\"\"\"\n",
    "    util = util.view(-1, 2)\n",
    "    pairs = torch.arange(util.numel()).view(-1, 2)\n",
    "    swap = util[:, 0] < util[:, 1]\n",
    "    pairs[swap] = pairs[swap].flip(1)\n",
    "    return pairs\n",
    "\n",
    "def gen_rand_X(problem, n: int) -> torch.Tensor:\n",
    "    return draw_sobol_samples(bounds=problem.bounds, n=1, q=n).squeeze(0)\n",
    "\n",
    "def generate_random_pref_data(\n",
    "    outcome_model: SingleTaskGP, n: int\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    X = gen_rand_X(problem, 2 * n)\n",
    "    Y = outcome_model.posterior(X).rsample().squeeze(0)\n",
    "    comps = gen_comps(neg_l1_dist(Y))\n",
    "    return Y, comps\n",
    "\n",
    "# -- Model‐fitting helpers ---------------------------------------------------\n",
    "\n",
    "def fit_outcome_model(X: torch.Tensor, Y: torch.Tensor) -> SingleTaskGP:\n",
    "    m = SingleTaskGP(\n",
    "        train_X=X, train_Y=Y,\n",
    "        input_transform=Normalize(d=X.size(-1)),\n",
    "        outcome_transform=Standardize(m=Y.size(-1)),\n",
    "    )\n",
    "    mll = ExactMarginalLogLikelihood(m.likelihood, m)\n",
    "    fit_gpytorch_model(mll)\n",
    "    return m\n",
    "\n",
    "def fit_pref_model(Y: torch.Tensor, comps: torch.Tensor) -> PairwiseGP:\n",
    "    m = PairwiseGP(Y, comps, input_transform=Normalize(d=Y.size(-1)))\n",
    "    mll = PairwiseLaplaceMarginalLogLikelihood(m.likelihood, m)\n",
    "    fit_gpytorch_model(mll)\n",
    "    return m\n",
    "\n",
    "# -- BOPE Loop ---------------------------------------------------------------\n",
    "\n",
    "# 1) Problem setup\n",
    "torch.manual_seed(0)\n",
    "problem = DTLZ2(dim=5, num_objectives=4)\n",
    "util_func = neg_l1_dist\n",
    "\n",
    "# 2) Initialization\n",
    "#    a) Initial experiments\n",
    "X_obs, Y_obs = gen_rand_X(problem, 8), None\n",
    "Y_obs = problem(X_obs)\n",
    "\n",
    "#    b) Fit initial outcome GP\n",
    "outcome_model = fit_outcome_model(X_obs, Y_obs)\n",
    "\n",
    "#    c) Initial preference data (e.g., 4 comparisons)\n",
    "Y_pref, pair_indices = generate_random_pref_data(outcome_model, n=4)\n",
    "pref_model = fit_pref_model(Y_pref, pair_indices)\n",
    "\n",
    "# 3) BOPE iteration parameters\n",
    "N_ITER = 5               # total BOPE cycles\n",
    "BATCH_SIZE = 1           # q\n",
    "NUM_OUT_SAMPLES = 100    # f fantasies\n",
    "NUM_PREF_SAMPLES = 10    # g fantasies\n",
    "NUM_RESTARTS = 8\n",
    "RAW_SAMPLES = 64\n",
    "BATCH_LIMIT = 4\n",
    "d = problem.dim\n",
    "\n",
    "for it in range(1, N_ITER + 1):\n",
    "    print(f\"\\n=== BOPE Iteration {it} ===\")\n",
    "\n",
    "    # 4) Compute current best utility baseline\n",
    "    with torch.no_grad():\n",
    "        u_mean = pref_model.posterior(Y_pref).mean.squeeze(-1)\n",
    "    best_f = u_mean.max()\n",
    "\n",
    "    # 5) Build LearnedObjective and sampler\n",
    "    pref_obj = LearnedObjective(pref_model=pref_model, sample_shape=torch.Size([NUM_PREF_SAMPLES]))\n",
    "    sampler = SobolQMCNormalSampler(sample_shape=torch.Size([NUM_OUT_SAMPLES]))\n",
    "\n",
    "    # 6) Instantiate qExpectedImprovement for EIUU\n",
    "    acqf = qExpectedImprovement(\n",
    "        model=outcome_model,\n",
    "        best_f=best_f,\n",
    "        sampler=sampler,\n",
    "        objective=pref_obj,\n",
    "        prune_baseline=True,\n",
    "    )\n",
    "\n",
    "    # 7) Optimize acquisition to get new X\n",
    "    bounds = torch.stack([torch.zeros(d), torch.ones(d)])\n",
    "    X_next, acq_val = optimize_acqf(\n",
    "        acq_function=acqf,\n",
    "        bounds=bounds,\n",
    "        q=BATCH_SIZE,\n",
    "        num_restarts=NUM_RESTARTS,\n",
    "        raw_samples=RAW_SAMPLES,\n",
    "        options={\"batch_limit\": BATCH_LIMIT},\n",
    "        sequential=False,\n",
    "    )\n",
    "    print(f\"Acq value: {acq_val.item():.4f}\")\n",
    "\n",
    "    # 8) Evaluate true function\n",
    "    Y_next = problem(X_next)\n",
    "    print(f\"Y_next: {Y_next}\")\n",
    "\n",
    "    # 9) Augment outcome data\n",
    "    X_obs = torch.cat([X_obs, X_next], dim=0)\n",
    "    Y_obs = torch.cat([Y_obs, Y_next], dim=0)\n",
    "\n",
    "    # 10) Refit outcome GP\n",
    "    outcome_model = fit_outcome_model(X_obs, Y_obs)\n",
    "\n",
    "    # 11) Gather new preference pairs for Y_next\n",
    "    #     (replace with real DM queries)\n",
    "    new_pairs = []\n",
    "    for i in range(Y_next.size(0)):\n",
    "        j = torch.randint(0, Y_pref.size(0), (1,)).item()\n",
    "        new_pairs.append([j, Y_pref.size(0) + i])\n",
    "    new_pairs = torch.tensor(new_pairs, dtype=torch.long)\n",
    "\n",
    "    # 12) Augment preference data and refit\n",
    "    Y_pref = torch.cat([Y_pref, Y_next], dim=0)\n",
    "    pair_indices = torch.cat([pair_indices, new_pairs], dim=0)\n",
    "    pref_model = fit_pref_model(Y_pref, pair_indices)\n",
    "\n",
    "print(\"\\nBOPE loop complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb7329f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
